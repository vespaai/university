<?xml version="1.0" encoding="utf-8" ?>
<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root. -->
<services version="1.0" xmlns:deploy="vespa" xmlns:preprocess="properties" minimum-required-vespa-version="8.327.49">

  <container id="default" version="1.0">
    <clients>
      <!-- 
        Required for mTLS to still work, the below
        configuration is equivalent to using a single
        security/clients.pem
      -->
      <client id="mtls" permissions="read,write">
        <certificate file="security/clients.pem"/>
      </client>
    </clients>
    <document-api/>
    <!-- 
        Here you should define contailer components like: LLM Client, Embedder
    -->

    <search>
    <!-- 
        Here you should define serach chains like: local, openai
    -->
    </search>

    <!-- 
        Here you should define GPU node for LLM inference, modify nodes entry
    -->
    <nodes>
        <node hostalias="node1" />
    </nodes>
  </container>
  <content id="msmarco" version="1.0">
    <redundancy>1</redundancy>
    <documents>
      <document mode="index" type="passage"/>
    </documents>
    <nodes>
      <node hostalias="node1" distribution-key="0" />
    </nodes>
  </content>

</services>
