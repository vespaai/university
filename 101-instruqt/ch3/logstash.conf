input {
    file {
        # TODO: change this to the absolute path of the CSV file
        # path => "/PATH/TO/sample-apps/examples/training-artifacts/101/ch3/sales-data.csv"
        # don't store the state of the file, so that Logstash will read the file from the beginning on each restart
        sincedb_path => "/dev/null"
        start_position => "beginning"
    }
}

filter {
    csv {
        columns => ["date", "price", "tax", "item", "customer"]
        # skip the first line, which contains the column names
        skip_header => true
        separator => ","
        quote_char => '"'
    }

    # parse the date field (which looks like: 2024-09-18 08:00:00) into a UNIX timestamp
    date {
        match => ["date", "yyyy-MM-dd HH:mm:ss"]
        # this will go to our default @timestamp field
    }

    mutate {
        # add a new field with the date (from @timestamp) in UNIX timestamp format
        add_field => { "date_unix" => "%{+%s}" }
        # remove unnecessary fields (including the original "date" field)
        remove_field => ["message", "@timestamp", "@version", "event", "log", "host", "date"]
    }

    mutate {
        # rename the new "date_unix" field to "date"
        rename => { "date_unix" => "date" }
        # convert it to an integer
        convert => { "date" => "integer" }
    }
}

output {
    # stdout { codec => rubydebug }

    vespa_feed {
        vespa_url => "http://localhost:8080"
        namespace => "purchase"
        document_type => "purchase"
        # no need to specify an ID field, it will try to use the "id" field, which doesn't exist
        # so a random UUID will be generated
    }
}